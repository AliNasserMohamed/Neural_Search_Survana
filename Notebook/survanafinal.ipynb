{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers qdrant-client torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d46p9Q3PK0p2",
        "outputId": "9d386f0a-c9e9-4d76-9ea0-5b8b998fb868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.65.1)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.65.1)\n",
            "Requirement already satisfied: httpx[http2]>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (0.27.0)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.10.1)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.27.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (67.7.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (2.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]>=0.20.0->qdrant-client) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install FlagEmbedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pxOx_Fe7K4eJ",
        "outputId": "cce1c500-b82b-4d0b-8254-3212197b254e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: FlagEmbedding in /usr/local/lib/python3.10/dist-packages (1.2.10)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from FlagEmbedding) (2.3.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from FlagEmbedding) (4.42.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from FlagEmbedding) (2.20.0)\n",
            "Requirement already satisfied: accelerate>=0.20.1 in /usr/local/lib/python3.10/dist-packages (from FlagEmbedding) (0.32.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from FlagEmbedding) (3.0.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.1->FlagEmbedding) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->FlagEmbedding) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->FlagEmbedding) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->FlagEmbedding) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->FlagEmbedding) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->FlagEmbedding) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->FlagEmbedding) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->FlagEmbedding) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->FlagEmbedding) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->FlagEmbedding) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->FlagEmbedding) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->FlagEmbedding) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->FlagEmbedding) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->FlagEmbedding) (3.9.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->FlagEmbedding) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->FlagEmbedding) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->FlagEmbedding) (9.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->FlagEmbedding) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->FlagEmbedding) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->FlagEmbedding) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->FlagEmbedding) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->FlagEmbedding) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->FlagEmbedding) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.0->FlagEmbedding) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.0->FlagEmbedding) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.0->FlagEmbedding) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.0->FlagEmbedding) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->FlagEmbedding) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->FlagEmbedding) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->FlagEmbedding) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->FlagEmbedding) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->FlagEmbedding) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->FlagEmbedding) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->FlagEmbedding) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MzU-Qdx-K5IC",
        "outputId": "c813264f-88ec-45a3-9d11-00e5e496122a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Clone the Annoy repository\n",
        "subprocess.run([\"git\", \"clone\", \"https://github.com/spotify/annoy.git\"])\n",
        "\n",
        "# Install the Annoy package\n",
        "subprocess.run([\"pip\", \"install\", \"annoy\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQwUN8p0K8Y3",
        "outputId": "36e76a0d-72e9-4352-c6ad-d79ea3728309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', 'annoy'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import VectorParams, Distance, PointStruct\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models as rest\n",
        "from FlagEmbedding import FlagModel\n",
        "from qdrant_client.http.models import VectorParams, Distance\n",
        "from FlagEmbedding import FlagReranker\n",
        "import string\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "from qdrant_client.http.models import PointStruct, VectorParams, Distance\n",
        "from qdrant_client import QdrantClient\n",
        "from FlagEmbedding import FlagModel\n",
        "from qdrant_client.http.models import VectorParams, Distance\n",
        "#from splitingdata import Chunking\n",
        "import logging\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import string\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2KEXTGvLDU9",
        "outputId": "b1259e8d-3b80-438b-f518-acc19483fea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from annoy import AnnoyIndex\n",
        "import faiss\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import pickle"
      ],
      "metadata": {
        "id": "F-8Eo5CdLATp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Chunking(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def read_data(self, filename):\n",
        "        df = pd.read_csv(filename)\n",
        "        return df\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        pass\n",
        "\n",
        "    def clean_data(self, text):\n",
        "        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "        text = re.sub(url_pattern, '', text)\n",
        "        emoji_pattern = re.compile(r'[\\U00010000-\\U0010ffff]')\n",
        "        text = re.sub(emoji_pattern, '', text)\n",
        "        #text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        text = re.sub(r'(?<!\\w)(?<!\\.)[^\\w\\s]*', '', text)\n",
        "        return text\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "        transformed_X = X.copy()\n",
        "\n",
        "        transformed_X['description'] = transformed_X['description'].str.lower()\n",
        "        transformed_X['description'] = transformed_X['description'].apply(self.clean_data)\n",
        "\n",
        "        return transformed_X[\"description\"].values\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        self.fit(X, y)\n",
        "        return self.transform(X)\n",
        "\n",
        "    def spliting(self, data):\n",
        "        All_sentences = []\n",
        "        for text in data:\n",
        "            sentences = []\n",
        "            for sentence in text.split(\".\"):\n",
        "                if len(sentence) > 1:\n",
        "                    sentences.append(sentence.strip())\n",
        "            All_sentences.extend(sentences[0:-1])\n",
        "        return All_sentences\n"
      ],
      "metadata": {
        "id": "8igUEHxFJ163"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k0nRnj3Ji0B"
      },
      "outputs": [],
      "source": [
        "class SBERT_Embedder:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
        "        \"\"\"\n",
        "        Initialize the SBERT embedder with the specified model.\n",
        "        \"\"\"\n",
        "        self.embedder = SentenceTransformer(model_name)\n",
        "        self.vector_size = self.embedder.get_sentence_embedding_dimension()\n",
        "        logging.info(f\"Model '{model_name}' loaded with vector size {self.vector_size}.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess_text(text):\n",
        "        \"\"\"\n",
        "        Normalize and preprocess the text.\n",
        "        \"\"\"\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return text\n",
        "\n",
        "    def create_embedding_for_document(self, document):\n",
        "        \"\"\"\n",
        "        Create an embedding for a given document using the embedding model.\n",
        "        \"\"\"\n",
        "        document = self.preprocess_text(document)\n",
        "        embedding = self.embedder.encode(document).tolist()\n",
        "        logging.info(f\"Embedding created for document: {document}\")\n",
        "        return embedding\n",
        "\n",
        "    def create_embeddings_for_documents(self, documents, batch_size=32):\n",
        "        \"\"\"\n",
        "        Create embeddings for a list of documents, where each document is a list of sentences.\n",
        "        \"\"\"\n",
        "        # Concatenate sentences for each document\n",
        "        concatenated_documents = [' '.join([self.preprocess_text(sentence) for sentence in doc]) for doc in documents]\n",
        "        embeddings = self.embedder.encode(concatenated_documents, batch_size=batch_size).tolist()\n",
        "        logging.info(f\"Embeddings created for {len(documents)} documents.\")\n",
        "        return embeddings\n",
        "\n",
        "    def get_query_vector(self, query):\n",
        "        \"\"\"\n",
        "        Convert a query into a vector using the embedding model.\n",
        "        \"\"\"\n",
        "        query = self.preprocess_text(query)\n",
        "        query_vector = self.embedder.encode(query).tolist()\n",
        "        logging.info(f\"Query vector created for query: {query}\")\n",
        "        return query_vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BGE_Embedder:\n",
        "    def __init__(self, model_name=\"BAAI/bge-reranker-v2-m3\", vector_size=1024):\n",
        "        \"\"\"\n",
        "        Initialize the BGE embedder with the specified model.\n",
        "        \"\"\"\n",
        "        print(\"Uploading model\")\n",
        "        self.embedder = FlagModel(model_name, use_fp16=True)\n",
        "        self.vector_size = vector_size\n",
        "        logging.info(f\"Model '{model_name}' loaded with vector size {self.vector_size}.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess_text(text):\n",
        "        \"\"\"\n",
        "        Normalize and preprocess the text.\n",
        "        \"\"\"\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return text\n",
        "\n",
        "    def create_embedding_for_document(self, document):\n",
        "        \"\"\"\n",
        "        Create an embedding for a given document using the embedding model.\n",
        "        \"\"\"\n",
        "        document = self.preprocess_text(document)\n",
        "        embedding = self.embedder.encode(document)\n",
        "        logging.info(f\"Embedding created for document: {document}\")\n",
        "        return embedding\n",
        "\n",
        "    def create_embeddings_for_documents(self, documents, batch_size=32):\n",
        "        \"\"\"\n",
        "        Create embeddings for a list of documents, where each document is a list of sentences.\n",
        "        \"\"\"\n",
        "        # Concatenate sentences for each document\n",
        "        concatenated_documents = [' '.join([self.preprocess_text(sentence) for sentence in doc]) for doc in documents]\n",
        "        embeddings = self.embedder.encode(concatenated_documents, batch_size=batch_size).tolist()\n",
        "        logging.info(f\"Embeddings created for {len(documents)} documents.\")\n",
        "        return embeddings\n",
        "\n",
        "    def get_query_vector(self, query):\n",
        "        \"\"\"\n",
        "        Convert a query into a vector using the embedding model.\n",
        "        \"\"\"\n",
        "        query = self.preprocess_text(query)\n",
        "        query_vector = self.embedder.encode(query).tolist()\n",
        "        logging.info(f\"Query vector created for query: {query}\")\n",
        "        return query_vector"
      ],
      "metadata": {
        "id": "L7eEIct5J8Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Qdrant_VB:\n",
        "    def __init__(self, api_key, qdrant_url, collection_name, vector_size):\n",
        "        self.api_key = api_key\n",
        "        self.qdrant_url = qdrant_url\n",
        "        self.collection_name = collection_name\n",
        "        self.qdrant_client = QdrantClient(url=qdrant_url, api_key=api_key)\n",
        "        self.vector_size = vector_size\n",
        "\n",
        "    def create_qdrant_collection_if_not_exist(self):\n",
        "        \"\"\"\n",
        "        Create a Qdrant collection if it does not already exist.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not self.qdrant_client.collection_exists(self.collection_name):\n",
        "                self.qdrant_client.create_collection(\n",
        "                    collection_name=self.collection_name,\n",
        "                    vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE)\n",
        "                )\n",
        "                logging.info(f\"Collection '{self.collection_name}' created.\")\n",
        "            else:\n",
        "                logging.info(f\"Collection '{self.collection_name}' already exists.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error creating collection '{self.collection_name}': {e}\")\n",
        "\n",
        "    def add_embedding_to_qdrant(self, embedding_vector, document, doc_id):\n",
        "        \"\"\"\n",
        "        Add an embedding vector and corresponding document information to a Qdrant collection.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.qdrant_client.upsert(\n",
        "                collection_name=self.collection_name,\n",
        "                points=[PointStruct(\n",
        "                    id=doc_id,\n",
        "                    vector=embedding_vector,\n",
        "                    payload={\"text\": document}\n",
        "                )]\n",
        "            )\n",
        "            logging.info(f\"Document ID {doc_id} added to collection '{self.collection_name}'.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to add document ID {doc_id} to collection '{self.collection_name}': {e}\")\n",
        "\n",
        "    def search_similar_vectors(self, query_vector, limit=5):\n",
        "        \"\"\"\n",
        "        Search for vectors in Qdrant that are similar to the query vector.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return self.qdrant_client.search(\n",
        "                collection_name=self.collection_name, query_vector=query_vector, limit=limit\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error searching for similar vectors: {e}\")\n",
        "            return []\n",
        "\n",
        "    def retrieve_text_by_ids(self, ids):\n",
        "        \"\"\"\n",
        "        Retrieve the text of documents from Qdrant given their IDs.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            results = self.qdrant_client.retrieve(collection_name=self.collection_name, ids=ids)\n",
        "            return [result.payload[\"text\"] for result in results]\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error retrieving texts by IDs: {e}\")\n",
        "            return []"
      ],
      "metadata": {
        "id": "noxCFEOVJ9mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Annoy:\n",
        "    def __init__(self, vector_size, metric='angular'):\n",
        "        \"\"\"\n",
        "        Initialize the Annoy index with the specified vector size and distance metric.\n",
        "        \"\"\"\n",
        "        self.vector_size = vector_size\n",
        "        self.metric = metric\n",
        "        self.annoy_index = AnnoyIndex(self.vector_size, self.metric)\n",
        "\n",
        "    def add_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Add embeddings to the Annoy index.\n",
        "        \"\"\"\n",
        "        if not isinstance(embeddings, np.ndarray) or embeddings.ndim != 2:\n",
        "            raise ValueError(\"embeddings must be a 2D NumPy array\")\n",
        "\n",
        "        for i, vec in enumerate(embeddings):\n",
        "            self.annoy_index.add_item(i, vec)\n",
        "\n",
        "    def build_index(self, num_trees=10):\n",
        "        \"\"\"\n",
        "        Build the Annoy index with the specified number of trees.\n",
        "        \"\"\"\n",
        "        if not isinstance(num_trees, int) or num_trees <= 0:\n",
        "            raise ValueError(\"num_trees must be a positive integer\")\n",
        "\n",
        "        self.annoy_index.build(num_trees)\n",
        "\n",
        "    def save_index(self, index_path='annoy_index.ann'):\n",
        "        \"\"\"\n",
        "        Save the Annoy index to disk.\n",
        "        \"\"\"\n",
        "        if not isinstance(index_path, str):\n",
        "            raise ValueError(\"index_path must be a string\")\n",
        "\n",
        "        try:\n",
        "            self.annoy_index.save(index_path)\n",
        "            print(f\"Index saved to {index_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while saving the index: {e}\")\n",
        "\n",
        "    def load_index(self, index_path):\n",
        "        \"\"\"\n",
        "        Load the Annoy index from disk.\n",
        "        \"\"\"\n",
        "        if not isinstance(index_path, str):\n",
        "            raise ValueError(\"index_path must be a string\")\n",
        "\n",
        "        try:\n",
        "            self.annoy_index.load(index_path)\n",
        "            print(f\"Index loaded from {index_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while loading the index: {e}\")\n",
        "\n",
        "    def query(self, vector, n=10):\n",
        "        \"\"\"\n",
        "        Query the Annoy index to find the nearest neighbors of the given vector.\n",
        "\n",
        "        Parameters:\n",
        "        vector (list): The query vector.\n",
        "        n (int, optional): The number of nearest neighbors to return. Default is 10.\n",
        "\n",
        "        Returns:\n",
        "        list: The list of indices of the nearest neighbors.\n",
        "        \"\"\"\n",
        "        if not isinstance(vector, list):\n",
        "            raise ValueError(\"vector must be a list\")\n",
        "\n",
        "        return self.annoy_index.get_nns_by_vector(vector, n)\n"
      ],
      "metadata": {
        "id": "0yM-1iO6MyS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "class FaissIndexer:\n",
        "    \"\"\"\n",
        "    A class to create and manage a FAISS index for efficient similarity search.\n",
        "\n",
        "    Attributes:\n",
        "        dimension (int): The dimensionality of the vectors to be indexed.\n",
        "        index_type (str): The type of FAISS index to use ('Flat', 'IVFFlat', 'PQ').\n",
        "        nlist (int): The number of clusters to use for the 'IVFFlat' index type.\n",
        "        m (int): The number of subquantizers to use for the 'PQ' index type.\n",
        "        index (faiss.Index): The FAISS index object.\n",
        "\n",
        "    Methods:\n",
        "        create_index(): Creates and returns the FAISS index based on the specified type.\n",
        "        train(data): Trains the FAISS index with the provided data if the index type requires training.\n",
        "        add(data): Adds the provided data to the FAISS index.\n",
        "        search(query_vector, num_neighbors): Searches the FAISS index for the nearest neighbors of the query vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dimension, index_type='IVFFlat', nlist=100, m=8):\n",
        "        \"\"\"\n",
        "        Initializes the FaissIndexer with the given parameters.\n",
        "\n",
        "        Parameters:\n",
        "            dimension (int): The dimensionality of the vectors to be indexed.\n",
        "            index_type (str, optional): The type of FAISS index to use. Default is 'IVFFlat'.\n",
        "            nlist (int, optional): The number of clusters to use for the 'IVFFlat' index type. Default is 100.\n",
        "            m (int, optional): The number of subquantizers to use for the 'PQ' index type. Default is 8.\n",
        "        \"\"\"\n",
        "        self.dimension = dimension\n",
        "        self.index_type = index_type\n",
        "        self.nlist = nlist\n",
        "        self.m = m\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def create_index(self):\n",
        "        \"\"\"\n",
        "        Creates and returns the FAISS index based on the specified type.\n",
        "\n",
        "        Returns:\n",
        "            faiss.Index: The created FAISS index object.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If an unknown index type is specified.\n",
        "        \"\"\"\n",
        "        if self.index_type == 'Flat':\n",
        "            return faiss.IndexFlatL2(self.dimension)\n",
        "        elif self.index_type == 'IVFFlat':\n",
        "            quantizer = faiss.IndexFlatL2(self.dimension)\n",
        "            return faiss.IndexIVFFlat(quantizer, self.dimension, self.nlist)\n",
        "        elif self.index_type == 'PQ':\n",
        "            return faiss.IndexPQ(self.dimension, self.m, 8)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown index type: {self.index_type}\")\n",
        "\n",
        "    def train(self, data):\n",
        "        \"\"\"\n",
        "        Trains the FAISS index with the provided data if the index type requires training.\n",
        "\n",
        "        Parameters:\n",
        "            data (np.ndarray): The training data to be used for training the index.\n",
        "        \"\"\"\n",
        "        if hasattr(self.index, 'train'):\n",
        "            self.index.train(data)\n",
        "\n",
        "    def add(self, data):\n",
        "        \"\"\"\n",
        "        Adds the provided data to the FAISS index.\n",
        "\n",
        "        Parameters:\n",
        "            data (np.ndarray): The data to be added to the index.\n",
        "        \"\"\"\n",
        "        self.index.add(data)\n",
        "\n",
        "    def search(self, query_vector, num_neighbors):\n",
        "        \"\"\"\n",
        "        Searches the FAISS index for the nearest neighbors of the query vector.\n",
        "\n",
        "        Parameters:\n",
        "            query_vector (np.ndarray): The query vector to search for.\n",
        "            num_neighbors (int): The number of nearest neighbors to return.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The indices of the nearest neighbors.\n",
        "        \"\"\"\n",
        "        distances, indices = self.index.search(query_vector.reshape(1, -1), num_neighbors)\n",
        "        return indices[0]\n"
      ],
      "metadata": {
        "id": "OUK7yY6beLOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate chuncker\n",
        "custom_transformer = Chunking()\n",
        "df = custom_transformer.read_data(\"data.csv\")\n",
        "transformed_df = custom_transformer.fit_transform(df)\n",
        "documents = custom_transformer.spliting(transformed_df)\n",
        "\n",
        "print(\"creating Embedderr and Qdrant object \")\n",
        "\n",
        "### usage\n",
        "api_key = \"mt0L253URDvlCcmxCTEzmbjuHnKlFpI9zmB3URtsMwpd6OUuk2aM3Q\"\n",
        "qdrant_url = \"https://9620ee82-03f9-4f04-a642-b006b25e2c7c.us-east4-0.gcp.cloud.qdrant.io\"\n",
        "qdrant_object = Qdrant_VB(api_key, qdrant_url, \"new_collection\", 1024)\n",
        "print(\"Qdrant object created\")\n",
        "bge = BGE_Embedder()\n",
        "print(\"BGE created\")\n",
        "counter = 0\n",
        "# for doc in documents:\n",
        "#     counter += 1\n",
        "#     qdrant_object.create_qdrant_collection_if_not_exist()\n",
        "#     document_embedding = bge.create_embeddings_for_documents(doc)\n",
        "#     qdrant_object.add_embedding_to_qdrant(document_embedding, doc, counter)\n",
        "#     print(\"doc\", doc, \" added successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xms910loKB4P",
        "outputId": "ffee680f-7827-4cec-c79e-29f66959c1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating Embedderr and Qdrant object \n",
            "Qdrant object created\n",
            "Uploading model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at BAAI/bge-reranker-v2-m3 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BGE created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_and_save_embeddings(documents):\n",
        "    \"\"\"\n",
        "    Creates and saves embeddings for the provided documents using SBERT and BGE.\n",
        "\n",
        "    Parameters:\n",
        "        documents (list of str): List of documents to create embeddings for.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the SBERT and BGE embedders.\n",
        "    \"\"\"\n",
        "    sbert_embedder = SBERT_Embedder()\n",
        "    sbert_embeddings = sbert_embedder.create_embeddings_for_documents(documents)\n",
        "    sbert_embeddings_np = np.array(sbert_embeddings)\n",
        "    with open('SBERTData.pickle', 'wb') as f:\n",
        "        pickle.dump(sbert_embeddings_np, f)\n",
        "\n",
        "    bge_embedder = BGE_Embedder()\n",
        "    # data = []\n",
        "    # for document in documents:\n",
        "    #     bge_embeddings = bge_embedder.create_embedding_for_document(document)\n",
        "    #     data.append(bge_embeddings)\n",
        "    # bge_embeddings_np = np.array(data)\n",
        "    # with open('BGEData.pickle', 'wb') as f:\n",
        "    #     pickle.dump(bge_embeddings_np, f)\n",
        "    return sbert_embedder, bge_embedder\n",
        "\n",
        "def load_embeddings_based_on_choice(embedder_type):\n",
        "    \"\"\"\n",
        "    Loads embeddings from a file based on the selected embedder type.\n",
        "\n",
        "    Parameters:\n",
        "        embedder_type (str): The type of embedder ('SBERT' or 'BGE').\n",
        "        embedder: The embedder object used to get the vector size.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the document embeddings as a NumPy array and the vector size.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unsupported embedder type is specified.\n",
        "    \"\"\"\n",
        "    if embedder_type == \"SBERT\":\n",
        "        file_path = 'SBERTData.pickle'\n",
        "        embedder = SBERT_Embedder()\n",
        "        vector_size = embedder.vector_size\n",
        "        with open(file_path, 'rb') as f:\n",
        "            document_embeddings_np = pickle.load(f)\n",
        "    elif embedder_type == \"BGE\":\n",
        "        file_path = 'BGEData.pickle'\n",
        "        embedder = BGE_Embedder()\n",
        "        vector_size = embedder.vector_size\n",
        "        with open(file_path, 'rb') as f:\n",
        "            document_embeddings_np = pickle.load(f)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported embedder type: {embedder_type}\")\n",
        "    return document_embeddings_np, vector_size,embedder\n",
        "\n",
        "def create_index(query, embedder, indextype, document_embeddings_np, vector_size):\n",
        "    \"\"\"\n",
        "    Creates an index and searches for the nearest neighbors of a query.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The query string.\n",
        "        embedder: The embedder object used to create the query vector.\n",
        "        indextype (str): The type of index to use ('Annoy' or 'Faiss').\n",
        "        document_embeddings_np (np.ndarray): The document embeddings.\n",
        "        vector_size (int): The size of the embedding vectors.\n",
        "\n",
        "    Returns:\n",
        "        list: The indices of the nearest neighbors.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unsupported index type is specified.\n",
        "    \"\"\"\n",
        "    if indextype == \"ANNOY\":\n",
        "        Aindex = Annoy(vector_size)\n",
        "        Aindex.add_embeddings(document_embeddings_np)\n",
        "        Aindex.build_index(num_trees=100)\n",
        "        Aindex.save_index(index_path='annoy_index.ann')\n",
        "        query_vector = embedder.get_query_vector(query)\n",
        "        neighbors = Aindex.query(query_vector, n=10)\n",
        "    elif indextype == \"FAISS\":\n",
        "        Findex = FaissIndexer(vector_size)\n",
        "        Findex.create_index()\n",
        "        Findex.train(document_embeddings_np)\n",
        "        Findex.add(document_embeddings_np)\n",
        "        query_vector = np.array(embedder.get_query_vector(query))\n",
        "        neighbors = Findex.search(query_vector, num_neighbors=10)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported index type: {indextype}\")\n",
        "    return neighbors\n"
      ],
      "metadata": {
        "id": "IMDi2oQuMWlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbert_embedder,bge_embedder=create_and_save_embeddings(documents)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CWc2seaIMdgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_embeddings_np,vector_size,embedder=load_embeddings_based_on_choice(\"BGE\")\n",
        "neighbors=create_index(\"Learn how to design large-scale systems\", embedder,\"FAISS\",document_embeddings_np,vector_size)"
      ],
      "metadata": {
        "id": "H21kx2Dtbxpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa3dfb9-8fdd-46bd-991d-ee485625dfd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at BAAI/bge-reranker-v2-m3 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nNearest Neighbors:\")\n",
        "for neighbor in neighbors:\n",
        "    print(f\"Document {neighbor + 1}: {documents[neighbor]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2fa6E1_cqKK",
        "outputId": "5a80ad1e-4886-4473-87f8-f2905a8ad84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Nearest Neighbors:\n",
            "Document 1: learn how to design large-scale systems\n",
            "Document 1218: build forms from json schema\n",
            "Document 474: explorations in core\n",
            "Document 90: resume template for chinese program\n",
            "Document 181: resume template for chinese program\n",
            "Document 1086: provides dynamic roles based authorisation for node\n",
            "Document 2073: a swipeable cards interface\n",
            "Document 1530: implementation of efficientnet model\n",
            "Document 2264: a javascript framework building upon but not dependant on) prototype's class\n",
            "Document 1866: and adjust the sales solution system module 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "########################################################################################################################################"
      ],
      "metadata": {
        "id": "-gfWWB2GtIrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[\"learn how to design large-scale systems\",\n",
        "\"build forms from json schema\",\n",
        "\"explorations in core\",\n",
        "\"resume template for chinese program\",\n",
        "\"resume template for chinese program\",\n",
        "\"provides dynamic roles based authorisation for node\",\n",
        "\"a swipeable cards interface\",\n",
        "\" implementation of efficientnet model\",\n",
        "\"a javascript framework building upon but not dependant on) prototype's class\",\n",
        "\" and adjust the sales solution system module 1\"]"
      ],
      "metadata": {
        "id": "BXBkNLbBCQBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Flask\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXAotzUHdTc1",
        "outputId": "1784b363-8f9e-4026-dc65-2607f96999eb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask ngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkwLFX0N-GIb",
        "outputId": "c6a491a0-d153-4837-e385-e3e9333fc5ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Collecting ngrok\n",
            "  Downloading ngrok-1.3.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
            "Installing collected packages: ngrok\n",
            "Successfully installed ngrok-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('templates', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "LtX2MfGKkitw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask\n",
        "from flask import request\n",
        "from flask_ngrok import run_with_ngrok"
      ],
      "metadata": {
        "id": "80e3TSz7-QHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, render_template, request\n",
        "\n",
        "app = Flask(__name__)\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('search.html')\n",
        "\n",
        "@app.route('/search', methods=['POST'])\n",
        "def search():\n",
        "    query = request.form.get('query')\n",
        "    embedding_type = request.form.get('embedding')\n",
        "    similarity_method = request.form.get('similarity')\n",
        "\n",
        "    # Replace these functions with your actual implementations\n",
        "    document_embeddings_np, vector_size, embedder = load_embeddings_based_on_choice(embedding_type)\n",
        "    res = create_index(query, embedder, similarity_method, document_embeddings_np, vector_size)\n",
        "\n",
        "    # # Debug print to see received values\n",
        "    # print(f\"Query: {query}, Embedding: {embedding_type}, Similarity: {similarity_method}\")\n",
        "    # print(f\"Results: {results}\")\n",
        "    return render_template('result.html', results=res)\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "n2BiN_Biq-yb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514670c9-2d1b-4e2d-befa-612ea1201066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8bbqCqnt9yFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}